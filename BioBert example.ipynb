{"cells":[{"cell_type":"code","source":["# install needed packages\n!python -m pip install --upgrade pip\n!pip install pytorch-transformers\n!pip install transformers\n!pip install pytorch-pretrained-bert\n!pip install pytorch-nlp\n!pip install tensorflow\n!pip install wget"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2f026810-cc83-4e84-9212-c0f227d11aff","outputId":"3d71244a-2ebc-4192-91bd-e0e970cf2833","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"052236ae-5084-44db-a331-026b152e2e51"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# import standard modules\nimport os\nimport re\nimport shutil\nimport wget\nimport tensorflow\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport time\n%matplotlib inline\n\n# imports BERT modules\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\nfrom tqdm import tqdm, trange\nfrom pytorch_transformers import BertModel\n\n# set max width of displayer columns to none to see entire transciptions\npd.set_option('display.max_colwidth', None)    "],"metadata":{"id":"edbc9c82-fd88-48f3-86ba-befbe3566845","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"981bc38c-23e7-404f-9801-a1d12ae77f9a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# download bioBERT from its github\n!wget \"https://github.com/naver/biobert-pretrained/releases/download/v1.1-pubmed/biobert_v1.1_pubmed.tar.gz\" -O \"biobert_weights\"\n\n# unzip the biobert file\n!tar -xzf biobert_weights\n\n# convert biobert weights to pytorch, will get warning about about cuda if there is no nvidia gpu\n!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4b3d100-4873-4278-8cf5-7ab3bffc6c4a","outputId":"0c91c65f-9bba-473b-bbd4-3ba93f41fdfc","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ba62480-6da8-429e-879d-aaaa2a5b5087"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["## importing the dataset from the pratical NLP book github\nfile_mt = \"mtsamples.csv\"\n!wget \"https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch10/Data/mtsamples.csv\" -O \"mtsamples.csv\"\n\n#print out an example transcription\ndf = pd.read_csv(file_mt)\ndf.rename( columns={'Unnamed: 0':'id'}, inplace=True )\nprint(f\"\\n{df['transcription'][0]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77d66105-8fab-4f6e-aab3-2e6bd80ce0ab","outputId":"55a25c3a-d14b-4fa4-9759-0c0529e77867","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c99a3c4-9d3c-4de4-914d-e82132eb8921"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# specify GPU device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    n_gpu = torch.cuda.device_count()\n    torch.cuda.get_device_name(0)\n\nfrom sklearn.preprocessing import LabelEncoder\n\n#Encoding medical_specialty\nle = LabelEncoder()\ndf[\"medical_specialty\"] = le.fit_transform(df[\"medical_specialty\"])\ndf.head(1)"],"metadata":{"id":"eb091c36-21a0-4e08-a464-29a7b3720f9e","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ab08b03-1530-4195-a6e8-ec52959aef39"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["description = list(df['description'])\n# Tokenize with BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('biobert_v1.1_pubmed', do_lower_case=True)\n\n# Restricting the max size of Tokens to 512(BERT doest accept any more than this)\ntokenized_texts = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(t)+['[SEP]'] , description))\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0])\n\nclasses = list(df['medical_specialty'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"365bca78-4d17-4fda-b639-bf83685a6399"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Set the maximum sequence length. \nMAX_LEN = 128\n\n# Pad our input tokens so that everything has a uniform length\ninput_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\nattention_masks = []\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)"],"metadata":{"id":"ed52f5ac-ba9e-4fc9-8310-792c2a35405b","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23a9bf09-1c5c-4ac6-8341-f7f74e9d699f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["batch_size = 16\n\n# Use train_test_split to split our data into train and validation sets for training\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, classes, \n                                                            random_state=2020, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=2020, test_size=0.1)\n                                             \n# Convert all of our data into torch tensors, the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# Create an iterator of our data with torch DataLoader \ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\n#Loading pre trained BERT\nmodel = BertForSequenceClassification.from_pretrained('biobert_v1.1_pubmed', num_labels=40)#binary classification\n\nif torch.cuda.is_available():\n    print(model.cuda())\nelse:\n    print(model)"],"metadata":{"id":"60e306ed-96b8-40ac-bf95-ed4501e3f283","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2889357-338e-4a93-ae2f-4e20c339407f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# BERT fine-tuning parameters\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=2e-5,\n                     warmup=.1)\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ntorch.cuda.empty_cache() \n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n# Number of training epochs \nepochs = 2#4\n\n# BERT training loop\nfor _ in trange(epochs, desc=\"Epoch\"):  \n  \n  ## TRAINING\n  \n    # Set our model to training mode\n    model.train()  \n    # Tracking variables\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    # Train the data for one epoch\n    for step, batch in enumerate(train_dataloader):\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n        # Forward pass\n        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        train_loss_set.append(loss.item())    \n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n       \n  ## VALIDATION\n\n    # Put model in evaluation mode\n    model.eval()\n    # Tracking variables \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n        with torch.no_grad():\n          # Forward pass, calculate logit predictions\n            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n\n# plot training performance\nplt.figure(figsize=(15,8))\nplt.title(\"Training loss\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(train_loss_set)\nplt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e61a3566-7766-435e-935e-7803c1a9ccb3","outputId":"db164f0e-33f4-453d-d562-1e7797559409","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd023246-b6a2-4180-9388-f07775410780"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.9.7","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"BioBert for Multi Label AMD.ipynb","provenance":[]},"application/vnd.databricks.v1+notebook":{"notebookName":"BioBert example","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2912474722795183}},"nbformat":4,"nbformat_minor":0}
